{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c949fde-5f2c-4b61-9b22-d9509d205333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sang1\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e747b39-3caa-4827-8e65-cf165526e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.1-cp39-cp39-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sang1\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.1-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.2 MB 660.6 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.3/1.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.8/1.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70ec4fe-c636-4b20-9c42-a62dff3af785",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DataSet/OPenCVData/OpenCVPointCloud/PointcloudTpose.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m\n\u001b[0;32m     94\u001b[0m Length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 전처리\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m processed_pointcloud_data_Tpose, processed_skeleton_data_Tpose \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path_point_cloud_Tpose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path_skeleton_Tpose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m processed_pointcloud_data_Walk, processed_skeleton_data_Walk \u001b[38;5;241m=\u001b[39m preprocess_and_split_data(file_path_point_cloud_Walk, file_path_skeleton_Walk, Length)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# 데이터 정규화\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#scaler = StandardScaler()\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#processed_pointcloud_data_Walk = [scaler.fit_transform(np.array(frame).reshape(-1, 3)).reshape(-1, 3) for frame in processed_pointcloud_data_Walk]\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m#데이터 병합\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 66\u001b[0m, in \u001b[0;36mpreprocess_and_split_data\u001b[1;34m(point_cloud_path, skeleton_path, length)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_and_split_data\u001b[39m(point_cloud_path, skeleton_path, length):\n\u001b[0;32m     64\u001b[0m     \n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# PointCloud_Data_Walk 전처리\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     processed_pointcloud_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_point_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_cloud_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Skeleton_Data_Wlak 전처리\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     processed_skeleton_data \u001b[38;5;241m=\u001b[39m preprocess_skeleton_data(skeleton_path)\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36mpreprocess_point_cloud\u001b[1;34m(file_path, length)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_point_cloud\u001b[39m(file_path, length):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         data_point_cloud \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Extract 'C' information from the JSON file\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DataSet/OPenCVData/OpenCVPointCloud/PointcloudTpose.json'"
     ]
    }
   ],
   "source": [
    "# PointCloud_Data 전처리 함수\n",
    "def preprocess_point_cloud(file_path, length):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data_point_cloud = [json.loads(line) for line in file]\n",
    "\n",
    "    # Extract 'C' information from the JSON file\n",
    "    processed_data = []\n",
    "\n",
    "    for i in range(len(data_point_cloud)-1):\n",
    "        if len(data_point_cloud[i+1]['C']) > length:\n",
    "            data_point_cloud[i+1]['C'] = data_point_cloud[i+1]['C'][:length]\n",
    "        elif len(data_point_cloud[i+1]['C']) < length:\n",
    "            # Fill the missing part with 0\n",
    "            data_point_cloud[i+1]['C'] += [0] * (length - len(data_point_cloud[i+1]['C']))\n",
    "\n",
    "        processed_data.append(data_point_cloud[i+1]['C'])\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# skeleton_Data 전처리 함수\n",
    "def preprocess_skeleton_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        skeleton_data = json.load(file)\n",
    "\n",
    "    # Process skeleton data\n",
    "    result_list = []\n",
    "    for item in skeleton_data:\n",
    "        x_value = float(item['X']) * 0.001\n",
    "        y_value = float(item['Y']) * 0.001\n",
    "        z_value = float(item['Z']) * 0.001\n",
    "\n",
    "        result_list.append({'X': x_value, 'Y': y_value, 'Z': z_value})\n",
    "\n",
    "    i = 0\n",
    "    skeleton_data_list = []\n",
    "\n",
    "    for item in result_list:\n",
    "        k = [item['X'], item['Y'], item['Z']]\n",
    "        skeleton_data_list.append(k)\n",
    "        i += 1\n",
    "\n",
    "    # Divide the data into groups of 22 joints per time step\n",
    "    label_data = [skeleton_data_list[i:i+33] for i in range(0, len(skeleton_data_list), 33)]\n",
    "    \n",
    "    return label_data\n",
    "\n",
    "def downsample_data(pointcloud_data, skeleton_data):\n",
    "    if len(pointcloud_data) > len(skeleton_data):\n",
    "        downsample_rate = len(pointcloud_data) // len(skeleton_data)\n",
    "        downsampled_pointcloud_data = pointcloud_data[:len(skeleton_data) * downsample_rate:downsample_rate]\n",
    "        processed_pointcloud_data = downsampled_pointcloud_data\n",
    "        processed_skeleton_data = skeleton_data\n",
    "    else:\n",
    "        downsample_rate = len(skeleton_data) // len(pointcloud_data)\n",
    "        downsampled_skeleton_data = skeleton_data[:len(pointcloud_data) * downsample_rate:downsample_rate]\n",
    "        processed_pointcloud_data = pointcloud_data\n",
    "        processed_skeleton_data = downsampled_skeleton_data\n",
    "\n",
    "    return processed_pointcloud_data, processed_skeleton_data\n",
    "\n",
    "\n",
    "def preprocess_and_split_data(point_cloud_path, skeleton_path, length):\n",
    "    \n",
    "    # PointCloud_Data_Walk 전처리\n",
    "    processed_pointcloud_data = preprocess_point_cloud(point_cloud_path, length)\n",
    "\n",
    "    # Skeleton_Data_Wlak 전처리\n",
    "    processed_skeleton_data = preprocess_skeleton_data(skeleton_path)\n",
    "\n",
    "    print(len(processed_pointcloud_data), len(processed_skeleton_data))\n",
    "\n",
    "    # DataDownSampling\n",
    "    processed_pointcloud_data, processed_skeleton_data = downsample_data(processed_pointcloud_data, processed_skeleton_data)\n",
    "\n",
    "    print(len(processed_pointcloud_data), len(processed_skeleton_data))\n",
    "\n",
    "    # 데이터 정규화\n",
    "    scaler = StandardScaler()\n",
    "    processed_pointcloud_data = [scaler.fit_transform(np.array(frame).reshape(-1, 3)).reshape(-1, 3) for frame in processed_pointcloud_data]\n",
    "\n",
    "    return processed_pointcloud_data, processed_skeleton_data\n",
    "    \n",
    "# 포인트 클라우드 데이터 로드\n",
    "file_path_point_cloud_Tpose = 'DataSet/OPenCVData/OpenCVPointCloud/PointcloudTpose.json'\n",
    "file_path_point_cloud_Walk = 'DataSet/OPenCVData/OpenCVPointCloud/PointcloudWalk.json'\n",
    "\n",
    "# skeleton Data Load\n",
    "file_path_skeleton_Tpose = 'DataSet/OPenCVData/OpenCVSkeleton/SkeletonTpose.json'\n",
    "file_path_skeleton_Walk = 'DataSet/OPenCVData/OpenCVSkeleton/SkeletonWalk.json'\n",
    "\n",
    "\n",
    "# 원하는 길이로 변경\n",
    "Length = 1500\n",
    "    \n",
    "\n",
    "# 전처리\n",
    "processed_pointcloud_data_Tpose, processed_skeleton_data_Tpose = preprocess_and_split_data(file_path_point_cloud_Tpose, file_path_skeleton_Tpose, Length)\n",
    "processed_pointcloud_data_Walk, processed_skeleton_data_Walk = preprocess_and_split_data(file_path_point_cloud_Walk, file_path_skeleton_Walk, Length)\n",
    "\n",
    "\n",
    "# 데이터 정규화\n",
    "#scaler = StandardScaler()\n",
    "#processed_pointcloud_data_Walk = [scaler.fit_transform(np.array(frame).reshape(-1, 3)).reshape(-1, 3) for frame in processed_pointcloud_data_Walk]\n",
    "\n",
    "\n",
    "#데이터 병합\n",
    "Merged_processed_pointcloud_data = processed_pointcloud_data_Tpose + processed_pointcloud_data_Walk\n",
    "Merged_processed_skeleton_data = processed_skeleton_data_Tpose + processed_skeleton_data_Walk\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(Merged_processed_pointcloud_data, Merged_processed_skeleton_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CNN 모델 정의\n",
    "input_layer = layers.Input(shape=(len(Merged_processed_pointcloud_data[0]), 3))\n",
    "conv1d_layer = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
    "global_maxpool_layer = layers.GlobalMaxPooling1D()(conv1d_layer)\n",
    "dense_layer = layers.Dense(99, activation='linear')(global_maxpool_layer)\n",
    "dropout_layer = layers.Dropout(0.2)(dense_layer)\n",
    "reshape_layer = layers.Reshape((33, 3))(dropout_layer)\n",
    "\n",
    "# Additional layers\n",
    "conv1d_layer_2 = layers.Conv1D(128, 3, activation='relu', padding='same')(reshape_layer)\n",
    "global_maxpool_layer_2 = layers.GlobalMaxPooling1D()(conv1d_layer_2)\n",
    "dense_layer_2 = layers.Dense(99, activation='linear')(global_maxpool_layer_2)\n",
    "dropout_layer_2 = layers.Dropout(0.2)(dense_layer_2)\n",
    "reshape_layer_2 = layers.Reshape((33, 3))(dropout_layer_2)\n",
    "\n",
    "# Output layer\n",
    "output_layer = layers.Dense(3, activation='linear')(reshape_layer_2)\n",
    "# 모델 정의\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=500, batch_size=64, validation_data=(np.array(X_test), np.array(y_test)))\n",
    "\n",
    "# 모델 저장\n",
    "model.save('CNN_Model_Skeleton_OpenCV_Vf.h5')\n",
    "\n",
    "# 모델 로드\n",
    "loaded_model = load_model('CNN_Model_Skeleton_OpenCV_Vf.h5')\n",
    "\n",
    "# 모델 평가를 수행\n",
    "loaded_model.evaluate(np.array(X_test), np.array(y_test))\n",
    "\n",
    "# 모델 예측을 수행\n",
    "loaded_predictions = loaded_model.predict(np.array(X_test))\n",
    "\n",
    "# 모델 평가\n",
    "loss = model.evaluate(np.array(X_test), np.array(y_test))\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "predictions = model.predict(np.array(X_test))\n",
    "\n",
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13125f-712d-4cd1-8aac-a6d18cac6c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
